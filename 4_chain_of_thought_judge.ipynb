{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d5f6b9",
   "metadata": {},
   "source": [
    "## Phase 4 ‚Äî Multi-Aspect Judge with Reference Answers\n",
    "\n",
    "\n",
    "Judge evaluates against reference answer and custom rubrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23e0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from typing import Dict, List, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "MODEL_ANSWER = \"gpt-4o-mini\"   # responder\n",
    "MODEL_JUDGE  = \"gpt-4.1\"        # judge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a68546",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EvaluationRubric:\n",
    "    \"\"\"Define evaluation criteria\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    weight: float = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecace3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import M\n",
    "\n",
    "\n",
    "class MultiAspectJudge:\n",
    "    \"\"\"Advanced judge with reference answers and custom rubrics\"\"\"\n",
    "    \n",
    "    def __init__(self, rubrics: List[EvaluationRubric] = None):\n",
    "        self.rubrics = rubrics or self._default_rubrics()\n",
    "    \n",
    "    def _default_rubrics(self) -> List[EvaluationRubric]:\n",
    "        \"\"\"Default evaluation rubrics\"\"\"\n",
    "        return [\n",
    "            EvaluationRubric(\"accuracy\", \"Factual correctness and precision\", 2.0),\n",
    "            EvaluationRubric(\"completeness\", \"Coverage of all relevant aspects\", 1.5),\n",
    "            EvaluationRubric(\"clarity\", \"Clear and understandable explanation\", 1.0),\n",
    "            EvaluationRubric(\"coherence\", \"Logical flow and organization\", 1.0),\n",
    "            EvaluationRubric(\"conciseness\", \"Appropriate length without redundancy\", 0.8)\n",
    "        ]\n",
    "    \n",
    "    def generate_answer(self, question: str, context: str = \"\") -> str:\n",
    "        \"\"\"Generate an answer\"\"\"\n",
    "        prompt = f\"{context}\\n\\nQuestion: {question}\" if context else question\n",
    "        \n",
    "        message = client.chat.completions.create(\n",
    "            model=MODEL_ANSWER,\n",
    "            max_tokens=1000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return message.choices[0].message.content\n",
    "    \n",
    "    def evaluate_with_reference(\n",
    "        self, \n",
    "        question: str, \n",
    "        candidate_answer: str,\n",
    "        reference_answer: Optional[str] = None,\n",
    "        context: str = \"\"\n",
    "    ) -> Dict:\n",
    "        \"\"\"Evaluate answer against reference and rubrics\"\"\"\n",
    "        \n",
    "        # Build rubric description\n",
    "        rubric_text = \"\\n\".join([\n",
    "            f\"- {r.name.capitalize()} (weight: {r.weight}): {r.description}\"\n",
    "            for r in self.rubrics\n",
    "        ])\n",
    "        \n",
    "        reference_section = \"\"\n",
    "        if reference_answer:\n",
    "            reference_section = f\"\"\"\n",
    "Reference Answer (gold standard):\n",
    "{reference_answer}\n",
    "\n",
    "Compare the candidate answer to this reference.\n",
    "\"\"\"\n",
    "        \n",
    "        context_section = \"\"\n",
    "        if context:\n",
    "            context_section = f\"\"\"\n",
    "Context/Background:\n",
    "{context}\n",
    "\"\"\"\n",
    "        \n",
    "        judge_prompt = f\"\"\"You are an expert evaluator assessing an answer's quality.\n",
    "\n",
    "{context_section}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Candidate Answer:\n",
    "{candidate_answer}\n",
    "\n",
    "{reference_section}\n",
    "\n",
    "Evaluation Rubrics:\n",
    "{rubric_text}\n",
    "\n",
    "For each rubric criterion, provide:\n",
    "1. Score (1-10)\n",
    "2. Brief justification\n",
    "\n",
    "Also provide:\n",
    "- Overall weighted score\n",
    "- Key strengths (2-3 points)\n",
    "- Key weaknesses (2-3 points)\n",
    "- Specific suggestions for improvement\n",
    "- Alignment with reference (if provided): percentage 0-100\n",
    "\n",
    "Respond ONLY with valid JSON:\n",
    "{{\n",
    "    \"rubric_scores\": {{\n",
    "        \"accuracy\": {{\"score\": <number>, \"justification\": \"...\"}},\n",
    "        \"completeness\": {{\"score\": <number>, \"justification\": \"...\"}},\n",
    "        \"clarity\": {{\"score\": <number>, \"justification\": \"...\"}},\n",
    "        \"coherence\": {{\"score\": <number>, \"justification\": \"...\"}},\n",
    "        \"conciseness\": {{\"score\": <number>, \"justification\": \"...\"}}\n",
    "    }},\n",
    "    \"weighted_score\": <number>,\n",
    "    \"raw_average\": <number>,\n",
    "    \"reference_alignment\": <number or null>,\n",
    "    \"strengths\": [\"strength1\", \"strength2\"],\n",
    "    \"weaknesses\": [\"weakness1\", \"weakness2\"],\n",
    "    \"improvements\": [\"suggestion1\", \"suggestion2\"],\n",
    "    \"overall_assessment\": \"detailed paragraph\"\n",
    "}}\n",
    "\"\"\"\n",
    "        \n",
    "        message = client.chat.completions.create(\n",
    "            model=MODEL_JUDGE,\n",
    "            max_tokens=2000,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": judge_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        response_text = message.choices[0].message.content\n",
    "        \n",
    "        try:\n",
    "            if \"```json\" in response_text:\n",
    "                response_text = response_text.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in response_text:\n",
    "                response_text = response_text.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            return json.loads(response_text.strip())\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"error\": \"Failed to parse JSON\",\n",
    "                \"raw_response\": response_text\n",
    "            }\n",
    "    \n",
    "    def batch_evaluate(\n",
    "        self, \n",
    "        questions: List[str],\n",
    "        candidate_answers: List[str],\n",
    "        reference_answers: Optional[List[str]] = None\n",
    "    ) -> List[Dict]:\n",
    "        \"\"\"Evaluate multiple Q&A pairs\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, (question, candidate) in enumerate(zip(questions, candidate_answers)):\n",
    "            reference = reference_answers[i] if reference_answers else None\n",
    "            evaluation = self.evaluate_with_reference(question, candidate, reference)\n",
    "            results.append({\n",
    "                \"question\": question,\n",
    "                \"evaluation\": evaluation\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58de7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_detailed_evaluation(eval_result: Dict):\n",
    "    \"\"\"Pretty print detailed evaluation\"\"\"\n",
    "    if \"error\" in eval_result:\n",
    "        print(f\"‚ùå Error: {eval_result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üìä DETAILED EVALUATION REPORT\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Scores by rubric\n",
    "    print(\"\\nüìã RUBRIC SCORES:\")\n",
    "    for criterion, details in eval_result['rubric_scores'].items():\n",
    "        score = details['score']\n",
    "        bar = \"‚ñà\" * score + \"‚ñë\" * (10 - score)\n",
    "        print(f\"\\n  {criterion.upper():<15} [{bar}] {score}/10\")\n",
    "        print(f\"  ‚îî‚îÄ {details['justification']}\")\n",
    "    \n",
    "    # Overall scores\n",
    "    print(f\"\\nüéØ OVERALL SCORES:\")\n",
    "    print(f\"  Raw Average:     {eval_result['raw_average']:.2f}/10\")\n",
    "    print(f\"  Weighted Score:  {eval_result['weighted_score']:.2f}/10\")\n",
    "    \n",
    "    if eval_result.get('reference_alignment'):\n",
    "        alignment = eval_result['reference_alignment']\n",
    "        bar = \"‚ñà\" * (alignment // 10) + \"‚ñë\" * (10 - alignment // 10)\n",
    "        print(f\"  Reference Match: [{bar}] {alignment}%\")\n",
    "    \n",
    "    # Strengths\n",
    "    print(f\"\\n‚úÖ STRENGTHS:\")\n",
    "    for strength in eval_result['strengths']:\n",
    "        print(f\"  ‚Ä¢ {strength}\")\n",
    "    \n",
    "    # Weaknesses\n",
    "    print(f\"\\n‚ö†Ô∏è  WEAKNESSES:\")\n",
    "    for weakness in eval_result['weaknesses']:\n",
    "        print(f\"  ‚Ä¢ {weakness}\")\n",
    "    \n",
    "    # Improvements\n",
    "    print(f\"\\nüí° IMPROVEMENT SUGGESTIONS:\")\n",
    "    for improvement in eval_result['improvements']:\n",
    "        print(f\"  ‚Ä¢ {improvement}\")\n",
    "    \n",
    "    # Overall assessment\n",
    "    print(f\"\\nüìù OVERALL ASSESSMENT:\")\n",
    "    print(f\"  {eval_result['overall_assessment']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cb7724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PHASE 4: MULTI-ASPECT JUDGE WITH REFERENCE ANSWERS\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EXAMPLE 1: Evaluation with Reference Answer\n",
      "======================================================================\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ChatCompletion' object has no attribute 'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     14\u001b[39m reference = \u001b[33m\"\"\"\u001b[39m\u001b[33mGradient descent is an optimization algorithm used to minimize \u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m    a cost function by iteratively moving in the direction of steepest descent \u001b[39m\n\u001b[32m     16\u001b[39m \u001b[33m    as defined by the negative of the gradient. It updates parameters by \u001b[39m\n\u001b[32m     17\u001b[39m \u001b[33m    subtracting the gradient multiplied by a learning rate.\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Generate a candidate answer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m candidate = \u001b[43mjudge\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ùì Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìö Reference Answer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mreference\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mMultiAspectJudge.generate_answer\u001b[39m\u001b[34m(self, question, context)\u001b[39m\n\u001b[32m     22\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mQuestion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;28;01melse\u001b[39;00m question\n\u001b[32m     24\u001b[39m message = client.chat.completions.create(\n\u001b[32m     25\u001b[39m     model=MODEL_ANSWER,\n\u001b[32m     26\u001b[39m     max_tokens=\u001b[32m1000\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     ]\n\u001b[32m     30\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m[\u001b[32m0\u001b[39m].text\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/AgenticAI/how-to-build-ent-agents/agentenv/lib/python3.12/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'ChatCompletion' object has no attribute 'content'"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"PHASE 4: MULTI-ASPECT JUDGE WITH REFERENCE ANSWERS\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "judge = MultiAspectJudge()\n",
    "    \n",
    "# Example 1: Evaluation with reference answer\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 1: Evaluation with Reference Answer\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "question = \"What is gradient descent?\"\n",
    "    \n",
    "reference = \"\"\"Gradient descent is an optimization algorithm used to minimize \n",
    "    a cost function by iteratively moving in the direction of steepest descent \n",
    "    as defined by the negative of the gradient. It updates parameters by \n",
    "    subtracting the gradient multiplied by a learning rate.\"\"\"\n",
    "    \n",
    "# Generate a candidate answer\n",
    "candidate = judge.generate_answer(question)\n",
    "    \n",
    "print(f\"\\n‚ùì Question: {question}\\n\")\n",
    "print(f\"üìö Reference Answer:\\n{reference}\\n\")\n",
    "print(f\"üí¨ Candidate Answer:\\n{candidate}\\n\")\n",
    "    \n",
    "# Evaluate\n",
    "evaluation = judge.evaluate_with_reference(question, candidate, reference)\n",
    "print_detailed_evaluation(evaluation)\n",
    "    \n",
    "# Example 2: Batch evaluation\n",
    "print(\"\\n\\n\" + \"=\" * 70)\n",
    "print(\"EXAMPLE 2: Batch Evaluation\")\n",
    "print(\"=\" * 70)\n",
    "    \n",
    "questions = [\n",
    "        \"What is overfitting?\",\n",
    "        \"Explain the bias-variance tradeoff\"\n",
    "    ]\n",
    "    \n",
    "print(\"\\nüìù Generating candidate answers...\")\n",
    "candidates = [judge.generate_answer(q) for q in questions]\n",
    "    \n",
    "references = [\n",
    "        \"Overfitting occurs when a model learns training data too well, including noise, reducing generalization to new data.\",\n",
    "        \"Bias-variance tradeoff: high bias models underfit (too simple), high variance models overfit (too complex). Goal is to balance both.\"\n",
    "    ]\n",
    "    \n",
    "results = judge.batch_evaluate(questions, candidates, references)\n",
    "    \n",
    "for i, result in enumerate(results, 1):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"QUESTION {i}: {result['question']}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print_detailed_evaluation(result['evaluation'])\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Batch evaluation complete!\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
