{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1d5f6b9",
   "metadata": {},
   "source": [
    "## Phase 1 â€” Simplest LLM-as-Judge (Pass/Fail)\n",
    "\n",
    "\n",
    "- One LLM generates an answer, another LLM judges it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23e0eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])\n",
    "\n",
    "MODEL_ANSWER = \"gpt-4o-mini\"   # responder (cheap + fast)\n",
    "MODEL_JUDGE  = \"gpt-4.1\"        # judge (stronger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "766877b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str) -> str:\n",
    "    \n",
    "    \"\"\"LLM generates an answer to a question\"\"\"\n",
    "    message = client.chat.completions.create(\n",
    "        model=MODEL_ANSWER,\n",
    "        max_tokens=1000,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Answer this question concisely: {question}\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    return message.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83033135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_answer(question: str, answer: str) -> dict:\n",
    "    \n",
    "    \"\"\"LLM judges the quality of an answer\"\"\"\n",
    "    judge_prompt = f\"\"\"You are an expert judge evaluating the quality of an answer.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: {answer}\n",
    "\n",
    "Please evaluate this answer on the following criteria:\n",
    "1. Correctness: Is the answer factually correct?\n",
    "2. Completeness: Does it fully answer the question?\n",
    "3. Clarity: Is it clear and easy to understand?\n",
    "\n",
    "Provide:\n",
    "- A score from 1-10 for each criterion\n",
    "- Overall score (1-10)\n",
    "- Brief explanation\n",
    "\n",
    "Format your response as:\n",
    "Correctness: [score]/10\n",
    "Completeness: [score]/10\n",
    "Clarity: [score]/10\n",
    "Overall: [score]/10\n",
    "Explanation: [your explanation]\n",
    "\"\"\"\n",
    "    \n",
    "    message = client.chat.completions.create(\n",
    "        model=MODEL_JUDGE,\n",
    "        max_tokens=1000,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": judge_prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"judgment\": message.choices[0].message.content,\n",
    "        \"raw_response\": message.choices[0].message.content\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7a046db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PHASE 1: BASIC LLM-AS-JUDGE\n",
      "============================================================\n",
      "\n",
      "Question: What is the capital of France?\n",
      "\n",
      "Step 1: Generating answer...\n",
      "Answer: The capital of France is Paris.\n",
      "\n",
      "Step 2: Judging the answer...\n",
      "\n",
      "Judgment:\n",
      "Correctness: 10/10  \n",
      "Completeness: 10/10  \n",
      "Clarity: 10/10  \n",
      "Overall: 10/10  \n",
      "Explanation: The answer is factually correct; Paris is indeed the capital of France. It fully addresses the question without omitting relevant information, and it is stated clearly and concisely, making it easy to understand.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of France?\"\n",
    "    \n",
    "print(\"=\" * 60)\n",
    "print(\"PHASE 1: BASIC LLM-AS-JUDGE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nQuestion: {question}\\n\")\n",
    "    \n",
    "# Step 1: Generate answer\n",
    "print(\"Step 1: Generating answer...\")\n",
    "answer = generate_answer(question)\n",
    "print(f\"Answer: {answer}\\n\")\n",
    "    \n",
    "# Step 2: Judge the answer\n",
    "print(\"Step 2: Judging the answer...\")\n",
    "judgment = judge_answer(question, answer)\n",
    "print(f\"\\nJudgment:\\n{judgment['judgment']}\")\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agentenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
